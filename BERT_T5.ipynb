{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "QGZWwQx8Nb3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from urllib import request\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def parse_conllu_using_pandas(block):\n",
        "    records = []\n",
        "    for line in block.splitlines():\n",
        "        if not line.startswith('#'):\n",
        "            records.append(line.strip().split('\\t'))\n",
        "    return pd.DataFrame.from_records(\n",
        "        records,\n",
        "        columns=['ID', 'FORM', 'TAG', 'Misc1', 'Misc2'])\n",
        "\n",
        "def tokens_to_labels(df):\n",
        "    return (\n",
        "        df.FORM.tolist(),\n",
        "        df.TAG.tolist()\n",
        "    )\n",
        "\n",
        "PREFIX = \"https://raw.githubusercontent.com/UniversalNER/\"\n",
        "DATA_URLS = {\n",
        "    \"en_ewt\": {\n",
        "        \"train\": \"UNER_English-EWT/master/en_ewt-ud-train.iob2\",\n",
        "        \"dev\": \"UNER_English-EWT/master/en_ewt-ud-dev.iob2\",\n",
        "        \"test\": \"UNER_English-EWT/master/en_ewt-ud-test.iob2\"\n",
        "    },\n",
        "    \"en_pud\": {\n",
        "        \"test\": \"UNER_English-PUD/master/en_pud-ud-test.iob2\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# en_ewt is the main train-dev-test split\n",
        "# en_pud is the OOD test set\n",
        "data_dict = defaultdict(dict)\n",
        "for corpus, split_dict in DATA_URLS.items():\n",
        "    for split, url_suffix in split_dict.items():\n",
        "        url = PREFIX + url_suffix\n",
        "        with request.urlopen(url) as response:\n",
        "            txt = response.read().decode('utf-8')\n",
        "            data_frames = map(parse_conllu_using_pandas,\n",
        "                              txt.split('\\n\\n'))\n",
        "            token_label_alignments = list(map(tokens_to_labels,\n",
        "                                              data_frames))\n",
        "            data_dict[corpus][split] = token_label_alignments\n",
        "\n",
        "# Saving the data so that you don't have to redownload it each time.\n",
        "with open('ner_data_dict.json', 'w', encoding='utf-8') as out:\n",
        "    json.dump(data_dict, out, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Each subset of each corpus is a list of tuples where each tuple\n",
        "# is a list of tokens with a corresponding list of labels.\n",
        "\n",
        "# Train on data_dict['en_ewt']['train']; validate on data_dict['en_ewt']['dev']\n",
        "# and test on data_dict['en_ewt']['test'] and data_dict['en_pud']['test']\n",
        "data_dict['en_ewt']['train'][0], data_dict['en_pud']['test'][1]"
      ],
      "metadata": {
        "id": "6jTb0GoxM3aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tap mapping\n",
        "\n",
        "tag_set = sorted({tag for tokens, labels in data_dict['en_ewt']['train'] for tag in labels})\n",
        "tag2id = {tag: i for i, tag in enumerate(tag_set)}\n",
        "id2tag = {i: tag for tag, i in tag2id.items()}\n"
      ],
      "metadata": {
        "id": "SDW0bSGOM5J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class (with BERT alignment)\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, data, tag2id):\n",
        "        self.data = data\n",
        "        self.tag2id = tag2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens, labels = self.data[idx]\n",
        "        encoding = tokenizer(tokens,\n",
        "                             is_split_into_words=True,\n",
        "                             truncation=True,\n",
        "                             padding='max_length',\n",
        "                             max_length=128,\n",
        "                             return_offsets_mapping=True)\n",
        "\n",
        "        word_ids = encoding.word_ids()\n",
        "        aligned_labels = []\n",
        "        prev_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)\n",
        "            elif word_idx != prev_word_idx:\n",
        "                aligned_labels.append(self.tag2id[labels[word_idx]])\n",
        "            else:\n",
        "                aligned_labels.append(self.tag2id[labels[word_idx]] if labels[word_idx].startswith(\"I-\") else -100)\n",
        "            prev_word_idx = word_idx\n",
        "\n",
        "        encoding.pop(\"offset_mapping\")\n",
        "        encoding[\"labels\"] = aligned_labels\n",
        "        return {k: torch.tensor(v) for k, v in encoding.items()}"
      ],
      "metadata": {
        "id": "lV0uKa9tNBH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "from transformers import BertForTokenClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2id)).to(device)\n",
        "\n",
        "train_dataset = NERDataset(data_dict[\"en_ewt\"][\"train\"], tag2id)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss = {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "XHlL3pnWNC0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set\n",
        "\n",
        "def extract_spans(labels, id2tag):\n",
        "    spans = []\n",
        "    span = []\n",
        "    label_type = None\n",
        "    for i, label_id in enumerate(labels):\n",
        "        tag = id2tag.get(label_id, \"O\")\n",
        "        if tag.startswith(\"B-\"):\n",
        "            if span:\n",
        "                spans.append((label_type, tuple(span)))\n",
        "            span = [i]\n",
        "            label_type = tag[2:]\n",
        "        elif tag.startswith(\"I-\") and label_type:\n",
        "            span.append(i)\n",
        "        else:\n",
        "            if span:\n",
        "                spans.append((label_type, tuple(span)))\n",
        "            span = []\n",
        "            label_type = None\n",
        "    if span:\n",
        "        spans.append((label_type, tuple(span)))\n",
        "    return spans"
      ],
      "metadata": {
        "id": "6dBmpI-HNGdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, tag2id, id2tag):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=8)\n",
        "    true_spans, pred_spans = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            labels = batch[\"labels\"].numpy()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                true = [l for l, m in zip(labels[i], batch[\"attention_mask\"][i]) if m == 1 and l != -100]\n",
        "                pred = [p for p, l, m in zip(preds[i], labels[i], batch[\"attention_mask\"][i]) if m == 1 and l != -100]\n",
        "                true_spans.extend(extract_spans(true, id2tag))\n",
        "                pred_spans.extend(extract_spans(pred, id2tag))\n",
        "\n",
        "    true_set = set(true_spans)\n",
        "    pred_set = set(pred_spans)\n",
        "    correct = true_set & pred_set\n",
        "\n",
        "    precision = len(correct) / len(pred_set) if pred_set else 0\n",
        "    recall = len(correct) / len(true_set) if true_set else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
        "\n",
        "    print(f\"Labelled Span Matching â€” Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "rbIG58aDNJdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "\n",
        "test_dataset = NERDataset(data_dict[\"en_ewt\"][\"test\"], tag2id)\n",
        "evaluate(model, test_dataset, tag2id, id2tag)\n"
      ],
      "metadata": {
        "id": "lRz4zgEjNMx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplify labels to B/I/O and create dataset\n",
        "\n",
        "def simplify_labels(labels):\n",
        "    return ['O' if tag == 'O' else tag.split('-')[0] for tag in labels]\n",
        "\n",
        "# Create new tag set and mappings\n",
        "simplified_tag_set = sorted({t for _, labels in data_dict['en_ewt']['train'] for t in simplify_labels(labels)})\n",
        "simpl_tag2id = {tag: i for i, tag in enumerate(simplified_tag_set)}\n",
        "simpl_id2tag = {i: tag for tag, i in simpl_tag2id.items()}\n",
        "\n",
        "# New dataset class using simplified labels\n",
        "class SimplifiedNERDataset(NERDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        tokens, labels = self.data[idx]\n",
        "        labels = simplify_labels(labels)  # <- key difference\n",
        "        encoding = tokenizer(tokens,\n",
        "                             is_split_into_words=True,\n",
        "                             truncation=True,\n",
        "                             padding='max_length',\n",
        "                             max_length=128,\n",
        "                             return_offsets_mapping=True)\n",
        "        word_ids = encoding.word_ids()\n",
        "        aligned_labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                aligned_labels.append(self.tag2id[labels[word_idx]])\n",
        "            else:\n",
        "                aligned_labels.append(self.tag2id[labels[word_idx]] if labels[word_idx] == 'I' else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        encoding.pop(\"offset_mapping\")\n",
        "        encoding[\"labels\"] = aligned_labels\n",
        "        return {k: torch.tensor(v) for k, v in encoding.items()}"
      ],
      "metadata": {
        "id": "0C6mEFVKNPL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate with per-label F1 and macro-F1\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def evaluate_with_f1(model, dataset, id2tag):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=8)\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            labels = batch[\"labels\"].numpy()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                true = [l for l, m in zip(labels[i], batch[\"attention_mask\"][i]) if m == 1 and l != -100]\n",
        "                pred = [p for p, l, m in zip(preds[i], labels[i], batch[\"attention_mask\"][i]) if m == 1 and l != -100]\n",
        "                all_labels.extend(true)\n",
        "                all_preds.extend(pred)\n",
        "\n",
        "    target_names = [id2tag[i] for i in sorted(id2tag)]\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, labels=list(range(len(target_names))), zero_division=0)\n",
        "\n",
        "    print(\"\\nPer-label performance:\")\n",
        "    for label, p, r, f in zip(target_names, precision, recall, f1):\n",
        "        print(f\"{label:<5}  P: {p:.3f}  R: {r:.3f}  F1: {f:.3f}\")\n",
        "\n",
        "    macro_f1 = sum(f1) / len(f1)\n",
        "    print(f\"\\nMacro-averaged F1: {macro_f1:.4f}\")"
      ],
      "metadata": {
        "id": "v7IJIS0GNUXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on simplified en_ewt test set\n",
        "\n",
        "simpl_test_dataset = SimplifiedNERDataset(data_dict[\"en_ewt\"][\"test\"], simpl_tag2id)\n",
        "evaluate_with_f1(model, simpl_test_dataset, simpl_id2tag)\n"
      ],
      "metadata": {
        "id": "ADA-xBvpNVgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Out-of-Domain (en_pud) test set\n",
        "\n",
        "simpl_pud_dataset = SimplifiedNERDataset(data_dict[\"en_pud\"][\"test\"], simpl_tag2id)\n",
        "evaluate_with_f1(model, simpl_pud_dataset, simpl_id2tag)"
      ],
      "metadata": {
        "id": "AzYrhppuNXn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "JQbFDpRyNaJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import Dataset as HFDataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "97qQErV4r-rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the NER Dataset for T5\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "class T5NERDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_input_length=128, max_target_length=64):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens, labels = self.data[idx]\n",
        "        input_text = \" \".join(tokens)\n",
        "        target_text = \" \".join(labels)\n",
        "\n",
        "        input_enc = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_input_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        target_enc = self.tokenizer(\n",
        "            target_text,\n",
        "            max_length=self.max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_enc[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_enc[\"input_ids\"].squeeze()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "6N-g4C4IsBb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets\n",
        "\n",
        "train_dataset = T5NERDataset(data_dict[\"en_ewt\"][\"train\"], tokenizer)\n",
        "val_dataset = T5NERDataset(data_dict[\"en_ewt\"][\"dev\"], tokenizer)\n",
        "test_dataset = T5NERDataset(data_dict[\"en_ewt\"][\"test\"], tokenizer)"
      ],
      "metadata": {
        "id": "XByu_M7esL5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "6OClczw5sbkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5-ner\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_dir=\"./logs\",\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "V4awtI6ksg1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "XhIRvv01so9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "49uLqCfjssHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "\n",
        "def evaluate_t5(model, dataset, tokenizer):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    for example in tqdm(dataset):\n",
        "        input_ids = example[\"input_ids\"].unsqueeze(0).to(device)\n",
        "        attention_mask = example[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)\n",
        "        pred = tokenizer.decode(output[0], skip_special_tokens=True).split()\n",
        "        true = tokenizer.decode(example[\"labels\"], skip_special_tokens=True).split()\n",
        "        preds.append(pred)\n",
        "        targets.append(true)\n",
        "\n",
        "    return preds, targets\n"
      ],
      "metadata": {
        "id": "KBbOV0lHuoUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the evaluation\n",
        "\n",
        "preds, targets = evaluate_t5(model, test_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "1pK1ZRCrurxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute token-level precision, recall, F1\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def flat_scores(preds, targets):\n",
        "    all_pred_labels = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for p, t in zip(preds, targets):\n",
        "        for pred_tag, true_tag in zip(p, t):\n",
        "            all_pred_labels.append(pred_tag)\n",
        "            all_true_labels.append(true_tag)\n",
        "\n",
        "    labels = sorted(set(all_true_labels) | set(all_pred_labels))\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_true_labels, all_pred_labels, labels=labels, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(\"Macro F1: {:.4f}, Precision: {:.4f}, Recall: {:.4f}\".format(f1, precision, recall))\n",
        "\n",
        "flat_scores(preds, targets)"
      ],
      "metadata": {
        "id": "QX1TrG3Kuw9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate T5 on en_pud Test Set\n",
        "\n",
        "pud_test_dataset = T5NERDataset(data_dict[\"en_pud\"][\"test\"], tokenizer)\n",
        "\n",
        "# Generate predictions\n",
        "pud_preds, pud_targets = evaluate_t5(model, pud_test_dataset, tokenizer)\n",
        "\n",
        "# Evaluate token-level performance\n",
        "flat_scores(pud_preds, pud_targets)"
      ],
      "metadata": {
        "id": "0Xxl2pobwOls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate T5 using Simplified Tags (B/I/O only)\n",
        "\n",
        "def simplify_bio(labels):\n",
        "    return ['O' if tag == 'O' else tag.split('-')[0] for tag in labels]\n",
        "\n",
        "def simplify_all(preds, targets):\n",
        "    simple_preds, simple_targets = [], []\n",
        "    for pred_seq, gold_seq in zip(preds, targets):\n",
        "        simple_preds.append(simplify_bio(pred_seq))\n",
        "        simple_targets.append(simplify_bio(gold_seq))\n",
        "    return simple_preds, simple_targets\n",
        "\n",
        "simple_preds, simple_targets = simplify_all(preds, targets)  # use en_ewt test preds\n",
        "flat_scores(simple_preds, simple_targets)\n",
        "\n",
        "simple_pud_preds, simple_pud_targets = simplify_all(pud_preds, pud_targets)\n",
        "flat_scores(simple_pud_preds, simple_pud_targets)"
      ],
      "metadata": {
        "id": "bsKgooRnwb32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Performance Comparison\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define model comparison data\n",
        "data = {\n",
        "    \"Model\": [\"BERT\", \"BERT\", \"T5\", \"T5\"],\n",
        "    \"Test Set\": [\"en_ewt\", \"en_pud\", \"en_ewt\", \"en_pud\"],\n",
        "    \"Macro F1\": [0.1226, 0.1757, 0.5316, 0.3940],\n",
        "    \"Precision\": [0.8830, 0.8960, 0.7512, 0.6526],\n",
        "    \"Recall\": [0.8356, 0.3690, 0.4687, 0.3731]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display table\n",
        "from IPython.display import display\n",
        "display(df)\n",
        "\n",
        "# Optionally save to CSV\n",
        "df.to_csv(\"bert_vs_t5_comparison.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "UR3ErowR0utX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Performance Comparison Plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Performance summary\n",
        "df = pd.DataFrame({\n",
        "    \"Model\": [\"BERT\", \"BERT\", \"T5\", \"T5\"],\n",
        "    \"Test Set\": [\"en_ewt\", \"en_pud\", \"en_ewt\", \"en_pud\"],\n",
        "    \"Macro F1\": [0.1226, 0.1757, 0.5316, 0.3940]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(df[\"Model\"] + \" (\" + df[\"Test Set\"] + \")\", df[\"Macro F1\"], color=[\"#1f77b4\", \"#1f77b4\", \"#ff7f0e\", \"#ff7f0e\"])\n",
        "plt.ylabel(\"Macro F1 Score\")\n",
        "plt.title(\"NER Macro F1 by Model and Test Set\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lBI_JcWWTfMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision vs Recall Scatter Plot\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Data\n",
        "performance = {\n",
        "    \"Model\": [\"BERT\", \"BERT\", \"T5\", \"T5\"],\n",
        "    \"Test Set\": [\"en_ewt\", \"en_pud\", \"en_ewt\", \"en_pud\"],\n",
        "    \"Precision\": [0.8830, 0.8960, 0.7512, 0.6526],\n",
        "    \"Recall\": [0.8356, 0.3690, 0.4687, 0.3731]\n",
        "}\n",
        "\n",
        "df_perf = pd.DataFrame(performance)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.scatterplot(data=df_perf, x=\"Recall\", y=\"Precision\", hue=\"Model\", style=\"Test Set\", s=100)\n",
        "plt.title(\"Precision vs Recall by Model and Domain\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BwtLecgaTlNF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}